{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment analysis divided into four class for considering robustness \n",
    "of the program:\n",
    "1.preprocessing_of_file\n",
    "2.classification_model\n",
    "3.prediction_savefile\n",
    "4.mean_absolute_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the libary function used for this project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all the libararies\n"
     ]
    }
   ],
   "source": [
    "# notebook magic function to plot figures within the notebook\n",
    "\n",
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#TfidfVectorizer from sklean\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Load datasets in the svmlight / libsvm format into sparse CSR matrix\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.svm import SVC\n",
    "from time import time \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "print(\"Successfully imported all the libararies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data successful\n"
     ]
    }
   ],
   "source": [
    "#load train dataset and test data set\n",
    "train_dataset, train_labels = load_svmlight_file('labeledBow_train.feat', 89527)\n",
    "test_dataset, test_labels = load_svmlight_file('labeledBow_test.feat', 89527)\n",
    "\n",
    "print (\"Loading Data successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class of Preprocessing of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitue is used as only two category either positive or negative\n",
    "class preprocessing_of_file:\n",
    "    # Binerize target data\n",
    "    # Converting target into binary like if review rating >5, positive (1) if <=5, negative (-1)\n",
    "    def binerize (self,target): \n",
    "        binary = []\n",
    "        for i in range(len(target)):\n",
    "            if target[i] > 5:\n",
    "                binary.append(1) # Positive\n",
    "            else:\n",
    "                binary.append(-1) # Negative\n",
    "        return binary\n",
    "    # Calculating Tf-Idf for training and testing\n",
    "    def tf_idf(self, training, testing): #taking trainig and testing arguments\n",
    "        tf_transformer = TfidfTransformer() #Transform a count matrix to a normalized tf or tf-idf representation\n",
    "        print(\"Training_data TF-IDF\")\n",
    "        #  It computes the TF for each review, the IDF using each review, and finally the TF-IDF for each review\n",
    "        training_tfidf = tf_transformer.fit_transform(training)\n",
    "        print(training_tfidf.shape)\n",
    "        print(\"Testing_data TF-IDF\")\n",
    "        # .transform on the testing data which computes the TF for each review, \n",
    "        # then the TF-IDF for each review using the IDF from the training data \n",
    "        testing_tfidf = tf_transformer.transform(testing)\n",
    "        print(testing_tfidf.shape)\n",
    "        return [training_tfidf,testing_tfidf]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preprocessing_of_file class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binerizing target ...\n",
      "Binerizing target data successfull\n",
      "\n",
      "\n",
      "Calculating Tf-Idf for training and testing\n",
      "Training_data TF-IDF\n",
      "(25000, 89527)\n",
      "Testing_data TF-IDF\n",
      "(25000, 89527)\n"
     ]
    }
   ],
   "source": [
    "# Object for preprocessing_of_file\n",
    "pf=preprocessing_of_file()\n",
    "print(\"Binerizing target ...\")\n",
    "train_label = pf.binerize(train_labels)\n",
    "test_label = pf.binerize(test_labels)\n",
    "print(\"Binerizing target data successfull\")\n",
    "print(\"\\n\")\n",
    "print(\"Calculating Tf-Idf for training and testing\")\n",
    "tfidf_data = pf.tf_idf(train_dataset, test_dataset)\n",
    "training_data = tfidf_data[0]\n",
    "testing_data = tfidf_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model Use for This project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_model:\n",
    "    \n",
    "    #Linear SVM classification Model function without cross validation \n",
    "    \n",
    "    def lSVM(self,training_data, training_target, testing_data, testing_target):\n",
    "        start = time()\n",
    "        # create a Linear SVC object\n",
    "        clf_linear = LinearSVC() \n",
    "        print(\"Data are Started Training\")\n",
    "        # teach the linear SVC using the training dataset\n",
    "        clf_linear.fit(training_data, train_label)\n",
    "        print(\"Training Successful\")\n",
    "        print(\"Data are Started Testing\")\n",
    "        # test the linear SVC accuracy using the testing dataset\n",
    "        clf_linear_accuracy = clf_linear.score(testing_data, test_label)*100\n",
    "        end = time()\n",
    "        return [clf_linear, round(clf_linear_accuracy,2), float(round(end-start))]\n",
    "    \n",
    "    #Linear SVM classification Model function with cross validation or c value adjustment   \n",
    "    def lSVC_para(self, training_data, training_target, testing_data, testing_target):\n",
    "        print(\"Calculating best parameter for LinearSVC Classifier ...\")\n",
    "        # array of C values to test\n",
    "        # Due to computation issue only taking value -2 to 4\n",
    "        clist = 2**np.array(range(-2, 4), dtype='float') # 4**-4, 4**-1, 2**0, 4**1, 4**2, ...\n",
    "        cvscores = [] #For High-Dimensional Data With Known Groups, Derive Scores For Plotting\n",
    "        iterator=range(8)\n",
    "        for c,r in zip(clist,iterator):\n",
    "            print(c)\n",
    "            clf= LinearSVC(C=c)\n",
    "            scores = cross_val_score(clf, training_data, training_target, cv=3) #Evaluate a score by cross-validation\n",
    "            print('Iteration #{}'.format(r+1))\n",
    "            print(\"score\", scores)\n",
    "            cvscores.append(scores.mean()*100)\n",
    "            bestscore, bestC = max([(val, clist[idx]) for (idx, val) in enumerate(cvscores)])\n",
    "        print('Best CV accuracy =', round(bestscore,2), '% achieved at C =', bestC)\n",
    "\n",
    "        # Retrain on whole trainning set using best C value obtained from Cross validation\n",
    "        print(\"Retrain on whole trainning set using best C value obtained from Cross validation\")\n",
    "        clf = LinearSVC(C=bestC)\n",
    "        clf.fit(training_data, training_target)\n",
    "        accu = clf.score(testing_data, testing_target)*100\n",
    "        return (clf, accu, bestC)\n",
    "            \n",
    "    #Random Forest classification Model function without adjust depth and number of tress\n",
    "    def random_forest(self, training_data, training_target, testing_data, testing_target):\n",
    "        start = time()\n",
    "        # create a andom Forest object\n",
    "        clf_forest = RandomForestClassifier()\n",
    "        print(\"Data are Started Training\")\n",
    "        # teach the andom Forest  using the training dataset\n",
    "        obj_random_forest=clf_forest.fit(training_data, train_label)\n",
    "        print(\"Training Successful\")\n",
    "        print(\"Data are Started Testing\")\n",
    "        # test the andom Forest  accuracy using the testing dataset\n",
    "        clf_forest_accuracy = clf_forest.score(testing_data, test_label)*100\n",
    "        end = time()\n",
    "        return [clf_forest, round(clf_forest_accuracy,2), float(round(end-start))]\n",
    "    def random_forest_para(self, training_data, training_target, testing_data, testing_target):\n",
    "        sample_leaf_options = [5, 10,25] # test leaf\n",
    "        depth_of_tress=[16,32,64] # test depths\n",
    "        best_accuracy=[]\n",
    "        for leaf,depth in zip(sample_leaf_options,depth_of_tress):\n",
    "            start = time()\n",
    "            # create a random Forest object\n",
    "            clf_forest = RandomForestClassifier(n_estimators = 100, min_samples_leaf=leaf, max_features='auto', max_depth=depth)\n",
    "            # teach the random Forest  using the training dataset\n",
    "            obj_random_forest=clf_forest.fit(training_data, train_label)\n",
    "            # test the random Forest  accuracy using the testing dataset\n",
    "            clf_forest_accuracy = clf_forest.score(testing_data, test_label)*100\n",
    "            end = time()\n",
    "            best_accuracy.append(clf_forest_accuracy)\n",
    "            print(\"Accuracy = \", round(clf_forest_accuracy,2), \"at\", \"Leaf Size = \",leaf, \"Tree Depth\", depth, \"% Time = \", float(round(end-start)),\"seconds\")\n",
    "        return [clf_forest, round(max(best_accuracy),2)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Classification Model without parameter adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Classifier \n",
      "Data are Started Training\n",
      "Training Successful\n",
      "Data are Started Testing\n",
      "Accuracy =  87.9 % Time =  1.0 seconds\n",
      "\n",
      "\n",
      "Random Forest Classifier without adjust depth and number of tress adjustment\n",
      "Data are Started Training\n",
      "Training Successful\n",
      "Data are Started Testing\n",
      "Accuracy =  73.2 % Time =  9.0 seconds\n"
     ]
    }
   ],
   "source": [
    "cm = classification_model() # Object for classification_model\n",
    "print(\"Linear SVM Classifier \")\n",
    "output = cm.lSVM(training_data, train_label, testing_data, test_label)\n",
    "obj_lSVM = output[0] \n",
    "print(\"Accuracy = \", output[1], \"% Time = \", output[2],\"seconds\")\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier without adjust depth and number of tress adjustment\")\n",
    "output = cm.random_forest(training_data, train_label, testing_data, test_label)\n",
    "obj_random_forest = output[0] \n",
    "print(\"Accuracy = \", output[1], \"% Time = \", output[2],\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Classification Model with parameter adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Classifier With Parameter Selection\n",
      "Calculating best parameter for LinearSVC Classifier ...\n",
      "0.25\n",
      "Iteration #1\n",
      "score [0.85805136 0.86177106 0.87013922]\n",
      "0.5\n",
      "Iteration #2\n",
      "score [0.85325174 0.85913127 0.8643783 ]\n",
      "1.0\n",
      "Iteration #3\n",
      "score [0.84485241 0.85157187 0.85417667]\n",
      "2.0\n",
      "Iteration #4\n",
      "score [0.83717303 0.84233261 0.84349496]\n",
      "4.0\n",
      "Iteration #5\n",
      "score [0.82889369 0.83273338 0.83713394]\n",
      "8.0\n",
      "Iteration #6\n",
      "score [0.82277418 0.82793377 0.83269323]\n",
      "Best CV accuracy = 86.33 % achieved at C = 0.25\n",
      "Retrain on whole trainning set using best C value obtained from Cross validation\n",
      "Accuracy =  88.628 % at Best C =  0.25 % Time =  16.0 seconds\n",
      "\n",
      "\n",
      "Random Forest Classifier with depth and number of tress adjustment\n",
      "Calculating best parameter for random  Classifier by changing tree depth and leaf\n",
      "Accuracy =  82.74 at Leaf Size =  5 Tree Depth 16 % Time =  6.0 seconds\n",
      "Accuracy =  83.09 at Leaf Size =  10 Tree Depth 32 % Time =  10.0 seconds\n",
      "Accuracy =  82.62 at Leaf Size =  25 Tree Depth 64 % Time =  7.0 seconds\n",
      "\n",
      "\n",
      "Best Accuracy =  83.09\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVM Classifier With Parameter Selection\")\n",
    "start = time()\n",
    "output = cm.lSVC_para(training_data, train_label, testing_data, test_label)\n",
    "end = time()\n",
    "obj_lSVM_para = output[0]\n",
    "print(\"Accuracy = \", output[1], \"% at Best C = \", output[2], \"% Time = \", float(round(end-start)),\"seconds\")\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier with depth and number of tress adjustment\")\n",
    "print(\"Calculating best parameter for random  Classifier by changing tree depth and leaf\")\n",
    "output = cm.random_forest_para(training_data, train_label, testing_data, test_label)\n",
    "obj_random_forest_para = output[0] \n",
    "print(\"\\n\")\n",
    "print(\"Best Accuracy = \", output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for prediction and save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_savefile:\n",
    "    # write Prediction function\n",
    "    def prediction(self, obj_clf):\n",
    "        pre = obj_clf.predict(testing_data)\n",
    "        print(\"Done\")\n",
    "        prediction_result = []\n",
    "        for i in range(len(pre)):\n",
    "            if pre[i] == 1:\n",
    "                prediction_result.append(str(i) + \", positive\") #label positve\n",
    "            else:\n",
    "                prediction_result.append(str(i) + \", negative\") \n",
    "        return(pre, prediction_result)\n",
    "    # Storing prediction in CSV file\n",
    "    def save_csv(self, prediction_result, fileName, labels):\n",
    "        print(\"Creating CSV file\")\n",
    "        # Open File\n",
    "        output_file = open(fileName+\".csv\",'w')\n",
    "        output_file.write(','.join(labels)+\"\\n\")\n",
    "        # Write data to file\n",
    "        for r in prediction_result:\n",
    "            output_file.write(r + \"\\n\")\n",
    "        output_file.close()\n",
    "        print(\"File saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction and save the file class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for new dataset from classifier...\n",
      "\n",
      "\n",
      "Using Linear SVM Prediction model\n",
      "Done\n",
      "Save Prediction result to CSV file\n",
      "Creating CSV file\n",
      "File saved!\n",
      "\n",
      "\n",
      "Using Random Forest Classification model\n",
      "Done\n",
      "Save Prediction result to CSV file\n",
      "Creating CSV file\n",
      "File saved!\n"
     ]
    }
   ],
   "source": [
    "ps=prediction_savefile() # Object for prediction_savefile\n",
    "print(\"Prediction for new dataset from classifier...\")\n",
    "print(\"\\n\")\n",
    "print (\"Using Linear SVM Prediction model\")\n",
    "pre_lsvm=ps.prediction(obj_lSVM_para)\n",
    "print(\"Save Prediction result to CSV file\")\n",
    "labels = [\"review\",\"rating\"]\n",
    "ps.save_csv(pre_lsvm[1], \"lsvm\", labels)\n",
    "print(\"\\n\")\n",
    "print (\"Using Random Forest Classification model\")\n",
    "pre_random=ps.prediction(obj_random_forest_para)\n",
    "print(\"Save Prediction result to CSV file\")\n",
    "labels = [\"review\",\"rating\"]\n",
    "ps.save_csv(pre_random[1], \"Random\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mean Absolute Percentage Error Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"The mean absolute percentage error (MAPE) is a measure of prediction accuracy of a forecasting method in statistics.\"\n",
    "class mean_absolute_percentage:\n",
    "    def mean_absolute_percentage_error(self, y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "#(Cross Validated 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate mean absulte percentage error\n",
      "Mean absulte percentage error of Linear SVM\n",
      "22.744\n",
      "Mean absulte percentage error of Random Forest\n",
      "34.768\n"
     ]
    }
   ],
   "source": [
    "#Calculate the test error for each classification model and display them on screen \n",
    "#The MAPE (Mean Absolute Percent Error) measures \n",
    "#the size of the error in percentage terms.\n",
    "print (\"Calculate mean absulte percentage error\")\n",
    "mp=mean_absolute_percentage()\n",
    "print(\"Mean absulte percentage error of Linear SVM\")\n",
    "print(mp.mean_absolute_percentage_error(test_label, pre_lsvm[0]))\n",
    "print(\"Mean absulte percentage error of Random Forest\")\n",
    "print(mp.mean_absolute_percentage_error(test_label, pre_random[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "                Table 1: Result of Sentiment Analysis\n",
    "\n",
    "|Model\t|Accuracy\t|MAPE\t|Time|                                                          \n",
    "|-------|-----------|-------|-------|-------|\n",
    "|Linear SVM|88.628\t|22.74|\t16 Second|\n",
    "|Random Forest Classifier|83.09\t|34.768\t|11 Second|\n",
    "\n",
    "NB: MAPE (mean_absolute_percentage error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Table 1, it is noticiable that Linear SVM gives better accuracy (88.628 %) with less MAPE only (22.74 %). It is obious that random forest works faster than SVM. Althogh some model could not able to use here as required large computation power for example  kernal SVM more than 10 minitues or more, more model function can be added in classification class and tested. However, Linear SVM is better than random forest model as data is categorized only two target features Positive and Negative. Therefore, it is the best model for sentiment analysis (Dr Ivan Bojicic 2018; Anon 2018). \n",
    "\n",
    "Reference:\n",
    "Dr Ivan Bojicic [University of Western Sydney] 2018, 301046 lecture Lecture 10 -\n",
    "More Predictive Modelling, 21 May).\n",
    "Anon, (2018). [online] Available at: https://www.researchgate.net/post/Is_random_forest_better_than_support_vector_machines [Accessed 3 Jun. 2018].\n",
    "Cross Validated, 2018. Mean absolute percentage error (MAPE) in Scikit-learn. [online] Cross Validated. Available at: https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn [Accessed 3 Jun. 2018]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
